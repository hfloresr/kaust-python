{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Overview](#Overview)\n",
    "\t* [Prepare](#Prepare)\n",
    "\t* [Links](#Links)\n",
    "\t* [This tutorial](#This-tutorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides multi-core and distributed parallel execution on larger-than-memory datasets.\n",
    "\n",
    "We can think of Dask at a high and a low level\n",
    "\n",
    "*  **High level collections:**  Dask provides high-level Array, Bag, and DataFrame\n",
    "   collections that mimic NumPy, lists, and Pandas but can operate in parallel on\n",
    "   datasets that don't fit into memory.  Dask's high-level collections are\n",
    "   alternatives to NumPy and Pandas for large datasets.\n",
    "*  **Low Level schedulers:** Dask provides dynamic task schedulers that\n",
    "   execute task graphs in parallel.  These execution engines power the\n",
    "   high-level collections mentioned above but can also power custom,\n",
    "   user-defined workloads.  These schedulers are low-latency (around 1ms) and\n",
    "   work hard to run computations in a small memory footprint.  Dask's\n",
    "   schedulers are an alternative to direct use of `threading` or\n",
    "   `multiprocessing` libraries in complex cases or other task scheduling\n",
    "   systems like `Luigi` or `IPython parallel`.\n",
    "\n",
    "Different users operate at different levels but it is useful to understand\n",
    "both.\n",
    "\n",
    "The Dask [use cases](http://dask.pydata.org/en/latest/use-cases.html) provides a number of sample workflows where Dask should be a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following core libraries\n",
    "\n",
    "    conda install numpy pandas h5py Pillow matplotlib scipy toolz pytables fastparquet\n",
    "\n",
    "And a recently updated version of Dask\n",
    "\n",
    "    conda install dask distributed\n",
    "\n",
    "The following is useful for task graph visualization\n",
    "```\n",
    "    conda install graphviz\n",
    "```\n",
    "\n",
    "You should clone this repository\n",
    "\n",
    "    git clone http://github.com/dask/dask-tutorial\n",
    "\n",
    "and then run this script to prepare artificial data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create random data for array exercise\n",
      "Create CSV accounts for dataframe exercise\n",
      "Create JSON accounts for bag exercise\n"
     ]
    }
   ],
   "source": [
    "# in directory dask-tutorial/\n",
    "# this takes a little while\n",
    "%run prep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Reference\n",
    "    *  [Documentation](https://dask.pydata.org/en/latest/)\n",
    "    *  [Code](https://github.com/dask/dask/)\n",
    "    *  [Blog](http://matthewrocklin.com/blog/)\n",
    "*  Ask for help\n",
    "    *   [dask](http://stackoverflow.com/questions/tagged/dask) tag on Stack Overflow\n",
    "    *   [github issues](https://github.com/dask/dask/issues/new) for bug reports and feature requests\n",
    "    *   [gitter](https://gitter.im/dask/dask) for quasi-realtime conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas there is a wealth of information in the documentation, linked above, here we aim to give practical advice to aid your understanding and application of Dask in everyday situations. This means that you should not expect every feature of Dask to be covered, but the examples hopefully are similar to the kinds of work-flows that you have in mind.\n",
    "\n",
    "The layout of the tutorial will be as follows:\n",
    "- Foundations: an explanation of what Dask is, how it works, and how to use lower-level primitives to set up computations. Casual users may wish to skip this section, although we consider it useful knowledge for all users.\n",
    "- Distributed: information on running Dask on the distributed scheduler, which enables scale-up to distributed settings and enhanced monitoring of task operations. The distributed scheduler is now generally the recommended engine for executing task work, even on single workstations or laptops.\n",
    "- Collections: convenient abstractions giving a familiar feel to big data\n",
    "    - bag: Python iterators with a functional paradigm, such as found in func/iter-tools and toolz - generalize lists/generators to big data; this will seem very familiar to users of PySpark's [RDD](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD)\n",
    "    - array: massive multi-dimensional numerical data, with Numpy functionality\n",
    "    - dataframes: massive tabular data, with Pandas functionality"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
